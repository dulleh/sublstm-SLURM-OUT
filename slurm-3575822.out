Running on host gpu10.bc4.acrc.priv
Slurm job ID is 3575822
This job runs on the following machines:
gpu10
| epoch   1 |   200/ 1327 batches | lr 0.001000 | ms/batch 23.90 | loss  6.79 | ppl   889.87
| epoch   1 |   400/ 1327 batches | lr 0.001000 | ms/batch 23.97 | loss  6.64 | ppl   766.02
| epoch   1 |   600/ 1327 batches | lr 0.001000 | ms/batch 24.03 | loss  6.66 | ppl   777.08
| epoch   1 |   800/ 1327 batches | lr 0.001000 | ms/batch 23.59 | loss  6.60 | ppl   733.95
| epoch   1 |  1000/ 1327 batches | lr 0.001000 | ms/batch 23.94 | loss  6.60 | ppl   736.08
| epoch   1 |  1200/ 1327 batches | lr 0.001000 | ms/batch 23.72 | loss  6.60 | ppl   735.62
Traceback (most recent call last):
  File "main.py", line 234, in <module>
    val_loss = evaluate(val_data)
  File "main.py", line 165, in evaluate
    output, hidden = model(data, hidden)
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/mnt/storage/home/mk17486/neural-networks/pytorch-subLSTM/tasks/word_language_model/model.py", line 72, in forward
    decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
