Running on host gpu16.bc4.acrc.priv
Slurm job ID is 3604246
This job runs on the following machines:
gpu16
Obtaining file:///mnt/storage/home/mk17486/neural-networks/pytorch-subLSTM
Requirement already satisfied: numpy in /mnt/storage/home/mk17486/.local/lib/python3.7/site-packages (from torch-sublstm==0.1.0) (1.18.1)
Requirement already satisfied: torch in /mnt/storage/home/mk17486/.local/lib/python3.7/site-packages (from torch-sublstm==0.1.0) (1.4.0)
Installing collected packages: torch-sublstm
  Found existing installation: torch-sublstm 0.1.0
    Uninstalling torch-sublstm-0.1.0:
      Successfully uninstalled torch-sublstm-0.1.0
  Running setup.py develop for torch-sublstm
Successfully installed torch-sublstm
subLSTMCuda
graph(%self : __torch__.subLSTM.basic.nn.SubLSTMCudaCell,
      %input.1 : Tensor,
      %state.1 : (Tensor, Tensor)):
  %11 : int = prim::Constant[value=1]() # ../../src/subLSTM/basic/nn.py:100:56
  %20 : int = prim::Constant[value=0]() # ../../src/subLSTM/basic/nn.py:102:81
  %old_h.1 : Tensor, %old_cell.1 : Tensor = prim::TupleUnpack(%state.1)
  %7 : Tensor = aten::detach(%old_h.1) # ../../src/subLSTM/basic/nn.py:100:23
  %9 : Tensor = aten::detach(%input.1) # ../../src/subLSTM/basic/nn.py:100:39
  %12 : Tensor[] = prim::ListConstruct(%7, %9)
  %X.1 : Tensor = aten::cat(%12, %11) # ../../src/subLSTM/basic/nn.py:100:12
  %X.3 : Tensor = aten::detach(%X.1) # ../../src/subLSTM/basic/nn.py:101:12
  %16 : Tensor = prim::GetAttr[name="bias"](%self)
  %17 : Tensor = aten::detach(%16) # ../../src/subLSTM/basic/nn.py:102:35
  %19 : Tensor = prim::GetAttr[name="weights"](%self)
  %21 : Tensor = aten::transpose(%19, %20, %11) # ../../src/subLSTM/basic/nn.py:102:58
  %22 : Tensor = aten::detach(%21) # ../../src/subLSTM/basic/nn.py:102:58
  %gate_weights.1 : Tensor = aten::addmm(%17, %X.3, %22, %11, %11) # ../../src/subLSTM/basic/nn.py:102:23
  %29 : bool = prim::requires_grad(%input.1)
  %30 : Tensor = prim::GetAttr[name="weights"](%self)
  %31 : bool = prim::requires_grad(%30)
  %32 : Tensor = prim::GetAttr[name="bias"](%self)
  %33 : bool = prim::requires_grad(%32)
  %35 : bool = prim::requires_grad(%old_h.1)
  %37 : bool = prim::requires_grad(%old_cell.1)
  %39 : bool = prim::requires_grad(%X.3)
  %41 : bool = prim::requires_grad(%gate_weights.1)
   = prim::Print(%29, %31, %33, %35, %37, %39, %41) # ../../src/subLSTM/basic/nn.py:104:8
  %43 : Tensor = prim::GetAttr[name="weights"](%self)
  %44 : Tensor = prim::GetAttr[name="bias"](%self)
  %49 : Tensor[] = prim::ListConstruct(%X.3, %gate_weights.1)
  %50 : Tensor[] = sublstm::apply(%input.1, %43, %44, %old_h.1, %old_cell.1, %49) # ../../src/subLSTM/basic/nn.py:107:26
  %new_h.1 : Tensor, %new_cell.1 : Tensor = prim::ListUnpack(%50)
  %55 : (Tensor, Tensor) = prim::TupleConstruct(%new_h.1, %new_cell.1)
  return (%55)

graph(%self : __torch__.subLSTM.basic.nn.SubLSTMCudaCell,
      %input.1 : Tensor,
      %state.1 : (Tensor, Tensor)):
  %11 : int = prim::Constant[value=1]() # ../../src/subLSTM/basic/nn.py:100:56
  %20 : int = prim::Constant[value=0]() # ../../src/subLSTM/basic/nn.py:102:81
  %old_h.1 : Tensor, %old_cell.1 : Tensor = prim::TupleUnpack(%state.1)
  %7 : Tensor = aten::detach(%old_h.1) # ../../src/subLSTM/basic/nn.py:100:23
  %9 : Tensor = aten::detach(%input.1) # ../../src/subLSTM/basic/nn.py:100:39
  %12 : Tensor[] = prim::ListConstruct(%7, %9)
  %X.1 : Tensor = aten::cat(%12, %11) # ../../src/subLSTM/basic/nn.py:100:12
  %X.3 : Tensor = aten::detach(%X.1) # ../../src/subLSTM/basic/nn.py:101:12
  %16 : Tensor = prim::GetAttr[name="bias"](%self)
  %17 : Tensor = aten::detach(%16) # ../../src/subLSTM/basic/nn.py:102:35
  %19 : Tensor = prim::GetAttr[name="weights"](%self)
  %21 : Tensor = aten::transpose(%19, %20, %11) # ../../src/subLSTM/basic/nn.py:102:58
  %22 : Tensor = aten::detach(%21) # ../../src/subLSTM/basic/nn.py:102:58
  %gate_weights.1 : Tensor = aten::addmm(%17, %X.3, %22, %11, %11) # ../../src/subLSTM/basic/nn.py:102:23
  %29 : bool = prim::requires_grad(%input.1)
  %30 : Tensor = prim::GetAttr[name="weights"](%self)
  %31 : bool = prim::requires_grad(%30)
  %32 : Tensor = prim::GetAttr[name="bias"](%self)
  %33 : bool = prim::requires_grad(%32)
  %35 : bool = prim::requires_grad(%old_h.1)
  %37 : bool = prim::requires_grad(%old_cell.1)
  %39 : bool = prim::requires_grad(%X.3)
  %41 : bool = prim::requires_grad(%gate_weights.1)
   = prim::Print(%29, %31, %33, %35, %37, %39, %41) # ../../src/subLSTM/basic/nn.py:104:8
  %43 : Tensor = prim::GetAttr[name="weights"](%self)
  %44 : Tensor = prim::GetAttr[name="bias"](%self)
  %49 : Tensor[] = prim::ListConstruct(%X.3, %gate_weights.1)
  %50 : Tensor[] = sublstm::apply(%input.1, %43, %44, %old_h.1, %old_cell.1, %49) # ../../src/subLSTM/basic/nn.py:107:26
  %new_h.1 : Tensor, %new_cell.1 : Tensor = prim::ListUnpack(%50)
  %55 : (Tensor, Tensor) = prim::TupleConstruct(%new_h.1, %new_cell.1)
  return (%55)

True True True False False False False
True True True False False False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
Traceback (most recent call last):
  File "main.py", line 232, in <module>
    train()
  File "main.py", line 198, in train
    loss.backward()
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: No grad accumulator for a saved leaf!
