Running on host gpu13.bc4.acrc.priv
Slurm job ID is 3604315
This job runs on the following machines:
gpu13
Obtaining file:///mnt/storage/home/mk17486/neural-networks/pytorch-subLSTM
Requirement already satisfied: numpy in /mnt/storage/home/mk17486/.local/lib/python3.7/site-packages (from torch-sublstm==0.1.0) (1.18.1)
Requirement already satisfied: torch in /mnt/storage/home/mk17486/.local/lib/python3.7/site-packages (from torch-sublstm==0.1.0) (1.4.0)
Installing collected packages: torch-sublstm
  Found existing installation: torch-sublstm 0.1.0
    Uninstalling torch-sublstm-0.1.0:
      Successfully uninstalled torch-sublstm-0.1.0
  Running setup.py develop for torch-sublstm
Successfully installed torch-sublstm
subLSTMCuda
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
1111
2222
3333
4444
aaaaa
bbbb
def forward(self,
    input: Tensor,
    state: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor]:
  old_h, old_cell, = state
  _0 = [torch.detach(old_h), torch.detach(input)]
  X = torch.cat(_0, 1)
  X0 = torch.detach(X)
  _1 = torch.detach(self.bias)
  _2 = torch.transpose(torch.detach(self.weights), 0, 1)
  gate_weights = torch.addmm(_1, X0, _2, beta=1, alpha=1)
  print(ops.prim.requires_grad(input), ops.prim.requires_grad(self.weights), ops.prim.requires_grad(self.bias), ops.prim.requires_grad(old_h), ops.prim.requires_grad(old_cell), ops.prim.requires_grad(X0), ops.prim.requires_grad(gate_weights))
  _3 = ops.sublstm.apply(input, self.weights, self.bias, old_h, old_cell, [X0, gate_weights])
  new_h, new_cell, = _3
  return (new_h, new_cell)

def forward(self,
    input: Tensor,
    state: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor]:
  old_h, old_cell, = state
  _0 = [torch.detach(old_h), torch.detach(input)]
  X = torch.cat(_0, 1)
  X0 = torch.detach(X)
  _1 = torch.detach(self.bias)
  _2 = torch.transpose(torch.detach(self.weights), 0, 1)
  gate_weights = torch.addmm(_1, X0, _2, beta=1, alpha=1)
  print(ops.prim.requires_grad(input), ops.prim.requires_grad(self.weights), ops.prim.requires_grad(self.bias), ops.prim.requires_grad(old_h), ops.prim.requires_grad(old_cell), ops.prim.requires_grad(X0), ops.prim.requires_grad(gate_weights))
  _3 = ops.sublstm.apply(input, self.weights, self.bias, old_h, old_cell, [X0, gate_weights])
  new_h, new_cell, = _3
  return (new_h, new_cell)

True True True False False False False
True True True False False False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
True True True True True False False
Traceback (most recent call last):
  File "main.py", line 232, in <module>
    train()
  File "main.py", line 198, in train
    loss.backward()
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: No grad accumulator for a saved leaf!
