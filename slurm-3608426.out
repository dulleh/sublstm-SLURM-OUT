Running on host gpu32.bc4.acrc.priv
Slurm job ID is 3608426
This job runs on the following machines:
gpu32
Obtaining file:///mnt/storage/home/mk17486/neural-networks/pytorch-subLSTM
Requirement already satisfied: numpy in /mnt/storage/home/mk17486/.local/lib/python3.7/site-packages (from torch-sublstm==0.1.0) (1.18.1)
Requirement already satisfied: torch in /mnt/storage/home/mk17486/.local/lib/python3.7/site-packages (from torch-sublstm==0.1.0) (1.4.0)
Installing collected packages: torch-sublstm
  Found existing installation: torch-sublstm 0.1.0
    Uninstalling torch-sublstm-0.1.0:
      Successfully uninstalled torch-sublstm-0.1.0
  Running setup.py develop for torch-sublstm
Successfully installed torch-sublstm
subLSTMCuda
def forward(self,
    input: Tensor,
    state: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor]:
  old_h, old_cell, = state
  _0 = [torch.detach(old_h), torch.detach(input)]
  X = torch.cat(_0, 1)
  X0 = torch.detach(X)
  _1 = torch.detach(self.bias)
  _2 = torch.transpose(torch.detach(self.weights), 0, 1)
  gate_weights = torch.addmm(_1, X0, _2, beta=1, alpha=1)
  _3 = ops.sublstm.apply(input, self.weights, self.bias, old_h, old_cell, [X0, gate_weights])
  new_h, new_cell, = _3
  return (new_h, new_cell)

def forward(self,
    input: Tensor,
    state: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor]:
  old_h, old_cell, = state
  _0 = [torch.detach(old_h), torch.detach(input)]
  X = torch.cat(_0, 1)
  X0 = torch.detach(X)
  _1 = torch.detach(self.bias)
  _2 = torch.transpose(torch.detach(self.weights), 0, 1)
  gate_weights = torch.addmm(_1, X0, _2, beta=1, alpha=1)
  _3 = ops.sublstm.apply(input, self.weights, self.bias, old_h, old_cell, [X0, gate_weights])
  new_h, new_cell, = _3
  return (new_h, new_cell)

Traceback (most recent call last):
  File "main.py", line 98, in <module>
    model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied, args.batch_size)
  File "/mnt/storage/home/mk17486/neural-networks/pytorch-subLSTM/tasks/word_language_model/model.py", line 40, in __init__
    batch_size=batch_size)).cuda()
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/jit/__init__.py", line 1255, in script
    return torch.jit._recursive.recursive_script(obj)
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/jit/_recursive.py", line 534, in recursive_script
    return create_script_module(nn_module, infer_methods_to_compile(nn_module))
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/jit/_recursive.py", line 296, in create_script_module
    return create_script_module_impl(nn_module, concrete_type, cpp_module, stubs)
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/jit/_recursive.py", line 340, in create_script_module_impl
    create_methods_from_stubs(concrete_type, stubs)
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/jit/_recursive.py", line 259, in create_methods_from_stubs
    concrete_type._create_methods(defs, rcbs, defaults)
RuntimeError: 
Variable 'hx' previously has type Optional[List[Tuple[Tensor, Tensor]]] but is now being assigned to a value of type List[Tensor]
:
  File "../../src/subLSTM/basic/nn.py", line 345

        if hx is None:
            hx = []
            ~~ <--- HERE
            for l in range(self.num_layers):
                # use input.new_zeros so dtype and device are the same as the input's

