Running on host gpu32.bc4.acrc.priv
Slurm job ID is 3604139
This job runs on the following machines:
gpu32
Obtaining file:///mnt/storage/home/mk17486/neural-networks/pytorch-subLSTM
Requirement already satisfied: numpy in /mnt/storage/home/mk17486/.local/lib/python3.7/site-packages (from torch-sublstm==0.1.0) (1.18.1)
Requirement already satisfied: torch in /mnt/storage/home/mk17486/.local/lib/python3.7/site-packages (from torch-sublstm==0.1.0) (1.4.0)
Installing collected packages: torch-sublstm
  Found existing installation: torch-sublstm 0.1.0
    Uninstalling torch-sublstm-0.1.0:
      Successfully uninstalled torch-sublstm-0.1.0
  Running setup.py develop for torch-sublstm
Successfully installed torch-sublstm
subLSTMCuda
graph(%self : __torch__.subLSTM.basic.nn.SubLSTMCudaCell,
      %input.1 : Tensor,
      %state.1 : (Tensor, Tensor)):
  %11 : int = prim::Constant[value=1]() # ../../src/subLSTM/basic/nn.py:100:56
  %20 : int = prim::Constant[value=0]() # ../../src/subLSTM/basic/nn.py:102:81
  %old_h.1 : Tensor, %old_cell.1 : Tensor = prim::TupleUnpack(%state.1)
  %7 : Tensor = aten::detach(%old_h.1) # ../../src/subLSTM/basic/nn.py:100:23
  %9 : Tensor = aten::detach(%input.1) # ../../src/subLSTM/basic/nn.py:100:39
  %12 : Tensor[] = prim::ListConstruct(%7, %9)
  %X.1 : Tensor = aten::cat(%12, %11) # ../../src/subLSTM/basic/nn.py:100:12
  %15 : bool = prim::requires_grad(%X.1)
   = prim::Print(%15) # ../../src/subLSTM/basic/nn.py:101:8
  %16 : Tensor = prim::GetAttr[name="bias"](%self)
  %17 : Tensor = aten::detach(%16) # ../../src/subLSTM/basic/nn.py:102:35
  %19 : Tensor = prim::GetAttr[name="weights"](%self)
  %21 : Tensor = aten::transpose(%19, %20, %11) # ../../src/subLSTM/basic/nn.py:102:58
  %22 : Tensor = aten::detach(%21) # ../../src/subLSTM/basic/nn.py:102:58
  %gate_weights.1 : Tensor = aten::addmm(%17, %X.1, %22, %11, %11) # ../../src/subLSTM/basic/nn.py:102:23
  %29 : Tensor = prim::GetAttr[name="weights"](%self)
  %30 : Tensor = prim::GetAttr[name="bias"](%self)
  %35 : Tensor[] = sublstm::apply(%input.1, %29, %30, %old_h.1, %old_cell.1, %X.1, %gate_weights.1) # ../../src/subLSTM/basic/nn.py:105:26
  %new_h.1 : Tensor, %new_cell.1 : Tensor = prim::ListUnpack(%35)
  %40 : (Tensor, Tensor) = prim::TupleConstruct(%new_h.1, %new_cell.1)
  return (%40)

graph(%self : __torch__.subLSTM.basic.nn.SubLSTMCudaCell,
      %input.1 : Tensor,
      %state.1 : (Tensor, Tensor)):
  %11 : int = prim::Constant[value=1]() # ../../src/subLSTM/basic/nn.py:100:56
  %20 : int = prim::Constant[value=0]() # ../../src/subLSTM/basic/nn.py:102:81
  %old_h.1 : Tensor, %old_cell.1 : Tensor = prim::TupleUnpack(%state.1)
  %7 : Tensor = aten::detach(%old_h.1) # ../../src/subLSTM/basic/nn.py:100:23
  %9 : Tensor = aten::detach(%input.1) # ../../src/subLSTM/basic/nn.py:100:39
  %12 : Tensor[] = prim::ListConstruct(%7, %9)
  %X.1 : Tensor = aten::cat(%12, %11) # ../../src/subLSTM/basic/nn.py:100:12
  %15 : bool = prim::requires_grad(%X.1)
   = prim::Print(%15) # ../../src/subLSTM/basic/nn.py:101:8
  %16 : Tensor = prim::GetAttr[name="bias"](%self)
  %17 : Tensor = aten::detach(%16) # ../../src/subLSTM/basic/nn.py:102:35
  %19 : Tensor = prim::GetAttr[name="weights"](%self)
  %21 : Tensor = aten::transpose(%19, %20, %11) # ../../src/subLSTM/basic/nn.py:102:58
  %22 : Tensor = aten::detach(%21) # ../../src/subLSTM/basic/nn.py:102:58
  %gate_weights.1 : Tensor = aten::addmm(%17, %X.1, %22, %11, %11) # ../../src/subLSTM/basic/nn.py:102:23
  %29 : Tensor = prim::GetAttr[name="weights"](%self)
  %30 : Tensor = prim::GetAttr[name="bias"](%self)
  %35 : Tensor[] = sublstm::apply(%input.1, %29, %30, %old_h.1, %old_cell.1, %X.1, %gate_weights.1) # ../../src/subLSTM/basic/nn.py:105:26
  %new_h.1 : Tensor, %new_cell.1 : Tensor = prim::ListUnpack(%35)
  %40 : (Tensor, Tensor) = prim::TupleConstruct(%new_h.1, %new_cell.1)
  return (%40)

True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
True
Traceback (most recent call last):
  File "main.py", line 232, in <module>
    train()
  File "main.py", line 198, in train
    loss.backward()
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: No grad accumulator for a saved leaf!
