Running on host gpu32.bc4.acrc.priv
Slurm job ID is 3608772
This job runs on the following machines:
gpu32
Obtaining file:///mnt/storage/home/mk17486/neural-networks/pytorch-subLSTM
Requirement already satisfied: numpy in /mnt/storage/home/mk17486/.local/lib/python3.7/site-packages (from torch-sublstm==0.1.0) (1.18.1)
Requirement already satisfied: torch in /mnt/storage/home/mk17486/.local/lib/python3.7/site-packages (from torch-sublstm==0.1.0) (1.4.0)
Installing collected packages: torch-sublstm
  Found existing installation: torch-sublstm 0.1.0
    Uninstalling torch-sublstm-0.1.0:
      Successfully uninstalled torch-sublstm-0.1.0
  Running setup.py develop for torch-sublstm
Successfully installed torch-sublstm
subLSTMCuda
def forward(self,
    input: Tensor,
    state: Optional[List[Tuple[Tensor, Tensor]]]) -> Tuple[Tensor, List[Tuple[Tensor, Tensor]]]:
  if self.batch_first:
    max_batch_size = torch.size(input, 0)
  else:
    max_batch_size = torch.size(input, 1)
  hx = annotate(List[Tuple[Tensor, Tensor]], [])
  if torch.__is__(state, None):
    for l in range(self.num_layers):
      _0 = [max_batch_size, self.hidden_size]
      _1 = torch.new_zeros(input, _0, dtype=None, layout=None, device=None, pin_memory=None)
      hidden = torch.detach(_1)
      _2 = torch.append(hx, (hidden, hidden))
    hx0 = hx
  else:
    hx0 = unchecked_cast(List[Tuple[Tensor, Tensor]], state)
  if self.batch_first:
    input0 = torch.contiguous(torch.transpose(input, 0, 1), memory_format=0)
  else:
    input0 = input
  timesteps = torch.size(input0, 0)
  outputs = []
  for i in range(timesteps):
    _3 = torch.append(outputs, torch.select(input0, 0, i))
  for time in range(timesteps):
    _4 = self.all_layers
    _5 = getattr(_4, "0")
    _6 = getattr(_4, "1")
    _7 = (self).flatten_parameters()
    _8 = (_5).forward(outputs[time], hx0[0], )
    out, c, = _8
    if self.use_dropout:
      out0 = (self.dropout).forward(out, )
    else:
      out0 = out
    _9 = torch._set_item(hx0, 0, (out0, c))
    _10 = torch._set_item(outputs, time, out0)
    _11 = (self).flatten_parameters()
    _12 = (_6).forward(outputs[time], hx0[1], )
    out1, c0, = _12
    if self.use_dropout:
      out2 = (self.dropout).forward(out1, )
    else:
      out2 = out1
    _13 = torch._set_item(hx0, 1, (out2, c0))
    _14 = torch._set_item(outputs, time, out2)
  out3 = torch.stack(outputs, 0)
  if self.batch_first:
    out4 = torch.transpose(out3, 0, 1)
  else:
    out4 = out3
  return (out4, hx0)

-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 79.80s | valid loss  5.71 | valid ppl   300.38
-----------------------------------------------------------------------------------------
Traceback (most recent call last):
  File "main.py", line 246, in <module>
    model.save('model', args.save)
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/jit/__init__.py", line 1626, in save
    return self._c.save(*args, **kwargs)
TypeError: save(): incompatible function arguments. The following argument types are supported:
    1. (self: torch._C.ScriptModule, filename: str, _extra_files: torch._C.ExtraFilesMap = ExtraFilesMap{}) -> None

Invoked with: <torch._C.ScriptModule object at 0x2b6115d235e0>, 'model', 'model.pt'
