Running on host gpu16.bc4.acrc.priv
Slurm job ID is 3604142
This job runs on the following machines:
gpu16
Obtaining file:///mnt/storage/home/mk17486/neural-networks/pytorch-subLSTM
Requirement already satisfied: numpy in /mnt/storage/home/mk17486/.local/lib/python3.7/site-packages (from torch-sublstm==0.1.0) (1.18.1)
Requirement already satisfied: torch in /mnt/storage/home/mk17486/.local/lib/python3.7/site-packages (from torch-sublstm==0.1.0) (1.4.0)
Installing collected packages: torch-sublstm
  Found existing installation: torch-sublstm 0.1.0
    Uninstalling torch-sublstm-0.1.0:
      Successfully uninstalled torch-sublstm-0.1.0
  Running setup.py develop for torch-sublstm
Successfully installed torch-sublstm
subLSTMCuda
graph(%self : __torch__.subLSTM.basic.nn.SubLSTMCudaCell,
      %input.1 : Tensor,
      %state.1 : (Tensor, Tensor)):
  %11 : int = prim::Constant[value=1]() # ../../src/subLSTM/basic/nn.py:100:56
  %22 : int = prim::Constant[value=0]() # ../../src/subLSTM/basic/nn.py:103:81
  %old_h.1 : Tensor, %old_cell.1 : Tensor = prim::TupleUnpack(%state.1)
  %7 : Tensor = aten::detach(%old_h.1) # ../../src/subLSTM/basic/nn.py:100:23
  %9 : Tensor = aten::detach(%input.1) # ../../src/subLSTM/basic/nn.py:100:39
  %12 : Tensor[] = prim::ListConstruct(%7, %9)
  %X.1 : Tensor = aten::cat(%12, %11) # ../../src/subLSTM/basic/nn.py:100:12
  %X.3 : Tensor = aten::detach(%X.1) # ../../src/subLSTM/basic/nn.py:101:12
  %17 : bool = prim::requires_grad(%X.3)
   = prim::Print(%17) # ../../src/subLSTM/basic/nn.py:102:8
  %18 : Tensor = prim::GetAttr[name="bias"](%self)
  %19 : Tensor = aten::detach(%18) # ../../src/subLSTM/basic/nn.py:103:35
  %21 : Tensor = prim::GetAttr[name="weights"](%self)
  %23 : Tensor = aten::transpose(%21, %22, %11) # ../../src/subLSTM/basic/nn.py:103:58
  %24 : Tensor = aten::detach(%23) # ../../src/subLSTM/basic/nn.py:103:58
  %gate_weights.1 : Tensor = aten::addmm(%19, %X.3, %24, %11, %11) # ../../src/subLSTM/basic/nn.py:103:23
  %31 : Tensor = prim::GetAttr[name="weights"](%self)
  %32 : Tensor = prim::GetAttr[name="bias"](%self)
  %37 : Tensor[] = sublstm::apply(%input.1, %31, %32, %old_h.1, %old_cell.1, %X.3, %gate_weights.1) # ../../src/subLSTM/basic/nn.py:106:26
  %new_h.1 : Tensor, %new_cell.1 : Tensor = prim::ListUnpack(%37)
  %42 : (Tensor, Tensor) = prim::TupleConstruct(%new_h.1, %new_cell.1)
  return (%42)

graph(%self : __torch__.subLSTM.basic.nn.SubLSTMCudaCell,
      %input.1 : Tensor,
      %state.1 : (Tensor, Tensor)):
  %11 : int = prim::Constant[value=1]() # ../../src/subLSTM/basic/nn.py:100:56
  %22 : int = prim::Constant[value=0]() # ../../src/subLSTM/basic/nn.py:103:81
  %old_h.1 : Tensor, %old_cell.1 : Tensor = prim::TupleUnpack(%state.1)
  %7 : Tensor = aten::detach(%old_h.1) # ../../src/subLSTM/basic/nn.py:100:23
  %9 : Tensor = aten::detach(%input.1) # ../../src/subLSTM/basic/nn.py:100:39
  %12 : Tensor[] = prim::ListConstruct(%7, %9)
  %X.1 : Tensor = aten::cat(%12, %11) # ../../src/subLSTM/basic/nn.py:100:12
  %X.3 : Tensor = aten::detach(%X.1) # ../../src/subLSTM/basic/nn.py:101:12
  %17 : bool = prim::requires_grad(%X.3)
   = prim::Print(%17) # ../../src/subLSTM/basic/nn.py:102:8
  %18 : Tensor = prim::GetAttr[name="bias"](%self)
  %19 : Tensor = aten::detach(%18) # ../../src/subLSTM/basic/nn.py:103:35
  %21 : Tensor = prim::GetAttr[name="weights"](%self)
  %23 : Tensor = aten::transpose(%21, %22, %11) # ../../src/subLSTM/basic/nn.py:103:58
  %24 : Tensor = aten::detach(%23) # ../../src/subLSTM/basic/nn.py:103:58
  %gate_weights.1 : Tensor = aten::addmm(%19, %X.3, %24, %11, %11) # ../../src/subLSTM/basic/nn.py:103:23
  %31 : Tensor = prim::GetAttr[name="weights"](%self)
  %32 : Tensor = prim::GetAttr[name="bias"](%self)
  %37 : Tensor[] = sublstm::apply(%input.1, %31, %32, %old_h.1, %old_cell.1, %X.3, %gate_weights.1) # ../../src/subLSTM/basic/nn.py:106:26
  %new_h.1 : Tensor, %new_cell.1 : Tensor = prim::ListUnpack(%37)
  %42 : (Tensor, Tensor) = prim::TupleConstruct(%new_h.1, %new_cell.1)
  return (%42)

False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
Traceback (most recent call last):
  File "main.py", line 232, in <module>
    train()
  File "main.py", line 198, in train
    loss.backward()
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/mnt/storage/home/mk17486/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: No grad accumulator for a saved leaf!
